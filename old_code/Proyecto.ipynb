{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036c138d",
   "metadata": {},
   "source": [
    "# Proyecto 13 - Quantifying Predictive Uncertainty in Autoregressive Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86596a52",
   "metadata": {},
   "source": [
    "Integrantes: \n",
    "\n",
    "- Benito Fuentes\n",
    "- Sebasti√°n Vergara"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda067a9",
   "metadata": {},
   "source": [
    "## 1. Introducci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6074406b",
   "metadata": {},
   "source": [
    "\n",
    "Los modelos de lenguaje autoregresivos, como GPT-2 o GPT-3, se han convertido en herramientas fundamentales en el procesamiento del lenguaje natural. Estos modelos predicen cada palabra de una secuencia bas√°ndose en las anteriores, generando texto coherente y fluido. Sin embargo, m√°s all√° de la calidad de las predicciones, a√∫n persiste un desaf√≠o clave: entender cu√°nta confianza tiene el modelo en lo que predice.\n",
    "\n",
    "En la pr√°ctica, los modelos no solo deben ser precisos, sino tambi√©n capaces de estimar su propia incertidumbre. En contextos donde los errores pueden ser cr√≠ticos ‚Äîcomo en sistemas de di√°logo, traducci√≥n autom√°tica o generaci√≥n de c√≥digo‚Äî, saber cu√°ndo el modelo est√° inseguro es tan importante como la respuesta misma.\n",
    "\n",
    "Para abordar esta problem√°tica, el proyecto busca cuantificar la incertidumbre predictiva en modelos de lenguaje mediante dos medidas provenientes de la teor√≠a de la informaci√≥n:\n",
    "\n",
    "Entrop√≠a: mide la dispersi√≥n o incertidumbre general en las predicciones del modelo.\n",
    "\n",
    "Surprisal: mide el grado de sorpresa o improbabilidad de cada token seg√∫n la distribuci√≥n predicha.\n",
    "\n",
    "A partir de estas m√©tricas, se pretende analizar c√≥mo var√≠a la confianza del modelo ante diferentes tipos de texto ‚Äîpor ejemplo, frases con una continuaci√≥n obvia frente a frases abiertas o ambiguas‚Äî y as√≠ comprender mejor c√≥mo razonan y calibran sus predicciones los modelos autoregresivos.\n",
    "\n",
    "En conjunto, el proyecto busca aportar una mirada interpretativa sobre el comportamiento interno de los modelos de lenguaje, desde una perspectiva basada en la teor√≠a de la informaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d056e64",
   "metadata": {},
   "source": [
    "## 2. Marco te√≥rico y revisi√≥n bibliogr√°fica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831b5e05",
   "metadata": {},
   "source": [
    "### 2.1. M√©tricas de informaci√≥n: entrop√≠a y surprisal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e3778",
   "metadata": {},
   "source": [
    "\n",
    "#### Entrop√≠a\n",
    "\n",
    "El concepto de **entrop√≠a** fue introducido por Claude Shannon (1948) en su trabajo *A Mathematical Theory of Communication* como una medida de la incertidumbre promedio asociada a una variable aleatoria.  \n",
    "Formalmente, para una distribuci√≥n discreta $ P(x) $:\n",
    "\n",
    "$\n",
    "H(X) = -\\sum_x P(x) \\log P(x)\n",
    "$\n",
    "\n",
    "En el contexto de los modelos de lenguaje autoregresivos, la entrop√≠a representa **cu√°n dispersa es la distribuci√≥n de probabilidades** que el modelo asigna a los posibles tokens siguientes.  \n",
    "- Una **entrop√≠a baja** indica que el modelo concentra su probabilidad en pocas opciones, reflejando **alta confianza**.  \n",
    "- Una **entrop√≠a alta** sugiere que el modelo distribuye la probabilidad entre muchas palabras posibles, lo que implica **mayor incertidumbre**.\n",
    "\n",
    "En unidades de bits (log base 2), la entrop√≠a mide el n√∫mero esperado de *preguntas s√≠/no* necesarias para identificar el pr√≥ximo token correcto.\n",
    "\n",
    "---\n",
    "\n",
    "#### Surprisal\n",
    "\n",
    "El **surprisal** (o ‚Äúsorpresa informacional‚Äù) mide cu√°n inesperado es un evento seg√∫n el modelo:\n",
    "\n",
    "$\n",
    "\\text{Surprisal}(x) = -\\log P(x)\n",
    "$\n",
    "\n",
    "En modelos de lenguaje, corresponde al negativo del logaritmo de la probabilidad asignada al token que efectivamente aparece en el texto.  \n",
    "- Tokens muy probables tienen surprisal bajo.  \n",
    "- Tokens inesperados (de baja probabilidad) generan surprisal alto.\n",
    "\n",
    "El surprisal refleja la sorpresa del modelo ante una observaci√≥n espec√≠fica, mientras que la entrop√≠a resume la incertidumbre global de toda la distribuci√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "#### Relaci√≥n entre entrop√≠a y surprisal\n",
    "\n",
    "Ambas m√©tricas est√°n √≠ntimamente relacionadas:  \n",
    "la entrop√≠a puede interpretarse como el valor esperado del surprisal bajo la distribuci√≥n de probabilidad.  \n",
    "Sin embargo, capturan aspectos complementarios:\n",
    "\n",
    "| M√©trica | Enfoca en... | Naturaleza |\n",
    "|----------|---------------|-------------|\n",
    "| **Entrop√≠a** | Incertidumbre global del modelo | Media sobre todas las predicciones |\n",
    "| **Surprisal** | Sorpresa frente a un token concreto | Valor puntual por observaci√≥n |\n",
    "\n",
    "En la pr√°ctica, el surprisal puede variar fuertemente entre tokens de una misma secuencia, mientras que la entrop√≠a var√≠a m√°s suavemente.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2. Aplicaciones de entrop√≠a y surprisal en modelos de lenguaje\n",
    "\n",
    "#### En ling√º√≠stica cognitiva\n",
    "\n",
    "La teor√≠a del surprisal ha sido ampliamente utilizada para modelar **procesos cognitivos humanos** durante la lectura.  \n",
    "Estudios emp√≠ricos muestran que palabras con alto surprisal tienden a provocar **mayores tiempos de lectura o mayor carga cognitiva**.\n",
    "\n",
    "- Smith & Levy (2013) demostraron que el surprisal predice con alta precisi√≥n los tiempos de lectura en ingl√©s, confirmando que la predicci√≥n ling√º√≠stica humana se ajusta a una escala log-probabil√≠stica.  \n",
    "  üìÑ *Smith, N. J., & Levy, R. (2013). The effect of word predictability on reading time is logarithmic.* Cognition, 128(3), 302‚Äì319. [https://doi.org/10.1016/j.cognition.2013.02.013](https://doi.org/10.1016/j.cognition.2013.02.013)\n",
    "\n",
    "- Goodkind & Bicknell (2018) mostraron que tanto el surprisal como la **reducci√≥n de entrop√≠a** explican de manera independiente la variabilidad en los tiempos de lectura.  \n",
    "  üìÑ *Goodkind, A., & Bicknell, K. (2018). Lexical predictability during natural reading: Effects of surprisal and entropy reduction.* Cognitive Science, 42(6), 1636‚Äì1672. [https://doi.org/10.1111/cogs.12623](https://doi.org/10.1111/cogs.12623)\n",
    "\n",
    "- Futrell et al. (2024) validan las predicciones de la teor√≠a del surprisal en m√∫ltiples idiomas, reforzando la universalidad de este fen√≥meno.  \n",
    "  üìÑ *Futrell, R., et al. (2024). Testing the Predictions of Surprisal Theory in 11 Languages.* Transactions of the Association for Computational Linguistics. [https://doi.org/10.1162/tacl_a_00612](https://doi.org/10.1162/tacl_a_00612)\n",
    "\n",
    "---\n",
    "\n",
    "#### En modelos de lenguaje y aprendizaje autom√°tico\n",
    "\n",
    "En los modelos de lenguaje modernos, la entrop√≠a y el surprisal se utilizan como **indicadores de incertidumbre y calibraci√≥n** del modelo.\n",
    "\n",
    "- Hou et al. (2024) proponen una descomposici√≥n de la incertidumbre total de los modelos de lenguaje en componentes **epist√©micos** (del modelo) y **aleatorios** (de los datos), usando t√©cnicas de *input clarification ensembling*.  \n",
    "  üìÑ *Hou, M., et al. (2024). Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling.* Proceedings of the 41st International Conference on Machine Learning (ICML). [https://proceedings.mlr.press/v235/hou24b.html](https://proceedings.mlr.press/v235/hou24b.html)\n",
    "\n",
    "- Liang et al. (2024) presentan una evaluaci√≥n sistem√°tica de distintos m√©todos de **cuantificaci√≥n de incertidumbre** para modelos de gran escala, comparando medidas basadas en entrop√≠a, probabilidad calibrada y m√©todos bayesianos.  \n",
    "  üìÑ *Liang, J., et al. (2024). Benchmarking Uncertainty Quantification Methods for Large Language Models.* Transactions of the Association for Computational Linguistics. [https://doi.org/10.1162/tacl_a_00737](https://doi.org/10.1162/tacl_a_00737)\n",
    "\n",
    "- Zhang et al. (2024) estudian c√≥mo estimar incertidumbre en escenarios de *in-context learning*, donde la fuente de incertidumbre proviene tanto del modelo como del contexto de entrada.  \n",
    "  üìÑ *Zhang, J., et al. (2024). Uncertainty Quantification for In-Context Learning of Large Language Models.* arXiv preprint. [https://arxiv.org/abs/2402.10189](https://arxiv.org/abs/2402.10189)\n",
    "\n",
    "- Zhao et al. (2024) ofrecen una revisi√≥n extensa sobre los desaf√≠os actuales y las aproximaciones te√≥ricas y pr√°cticas para cuantificar la incertidumbre en modelos de lenguaje de gran escala.  \n",
    "  üìÑ *Zhao, C., et al. (2024). A Survey on Uncertainty Quantification of Large Language Models.* arXiv preprint. [https://arxiv.org/abs/2410.15326](https://arxiv.org/abs/2410.15326)\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3. Cr√≠ticas, desaf√≠os y l√≠neas de investigaci√≥n\n",
    "\n",
    "A pesar de su relevancia, existen varios desaf√≠os en la aplicaci√≥n de estas m√©tricas a modelos de lenguaje modernos:\n",
    "\n",
    "- **Dependencia del tokenizador:** la granularidad de las unidades (palabras, subpalabras o caracteres) afecta las distribuciones de probabilidad y, por ende, los valores de entrop√≠a y surprisal.\n",
    "- **Correlaci√≥n entre m√©tricas:** en lenguaje natural, la entrop√≠a y el surprisal tienden a correlacionarse fuertemente, lo que dificulta aislar sus efectos individuales.\n",
    "- **Calibraci√≥n imperfecta:** las probabilidades de salida de los modelos no siempre representan bien la verdadera incertidumbre del modelo.\n",
    "- **Escalabilidad:** medir incertidumbre en modelos grandes implica un costo computacional elevado, especialmente si se analizan distribuciones completas de vocabulario.\n",
    "- **Fuentes mixtas de incertidumbre:** distinguir entre incertidumbre *epist√©mica* (por falta de conocimiento del modelo) y *aleatoria* (por variabilidad inherente en los datos) es un tema de investigaci√≥n activa.\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4. S√≠ntesis\n",
    "\n",
    "La entrop√≠a y el surprisal constituyen herramientas fundamentales para interpretar la **confianza y la predictibilidad** en los modelos de lenguaje.  \n",
    "Su uso se ha extendido desde la ling√º√≠stica cognitiva hasta la inteligencia artificial moderna, sirviendo como un puente entre la teor√≠a de la informaci√≥n y el aprendizaje profundo.  \n",
    "En este proyecto, ambas m√©tricas ser√°n empleadas para **analizar cuantitativamente el grado de certeza** del modelo frente a distintos tipos de texto, estableciendo las bases para los experimentos de los hitos posteriores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07bf057",
   "metadata": {},
   "source": [
    "## 3. Metodolog√≠a general del proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64276d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cdc73c0",
   "metadata": {},
   "source": [
    "## 4. Preparaci√≥n del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2b415",
   "metadata": {},
   "source": [
    "## 5. Implementaci√≥n de funciones base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2fa5b7",
   "metadata": {},
   "source": [
    "## 6. Preparaci√≥n y validaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cc6f95",
   "metadata": {},
   "source": [
    "## 7. Primeros experimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511ae63",
   "metadata": {},
   "source": [
    "## 8. An√°lisis completo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143181b",
   "metadata": {},
   "source": [
    "## 9. Conclusiones globales del proyecto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b5dc8",
   "metadata": {},
   "source": [
    "## 10. Anexos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b59ef6",
   "metadata": {},
   "source": [
    "## Referencias "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc9573",
   "metadata": {},
   "source": [
    "Goodkind, A., & Bicknell, K. (2018). Lexical predictability during natural reading: Effects of surprisal and entropy reduction. *Cognitive Science, 42*(6), 1636‚Äì1672. https://doi.org/10.1111/cogs.12623  \n",
    "\n",
    "Hou, M., Liu, H., Luo, L., & Zhang, Y. (2024). Decomposing uncertainty for large language models through input clarification ensembling. *Proceedings of the 41st International Conference on Machine Learning (ICML 2024).* PMLR. https://proceedings.mlr.press/v235/hou24b.html  \n",
    "\n",
    "Liang, J., Xie, Q., Kumar, A., & Song, D. (2024). Benchmarking uncertainty quantification methods for large language models. *Transactions of the Association for Computational Linguistics, 12*, 178‚Äì198. https://doi.org/10.1162/tacl_a_00737  \n",
    "\n",
    "Shannon, C. E. (1948). A mathematical theory of communication. *The Bell System Technical Journal, 27*(3), 379‚Äì423. https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf  \n",
    "\n",
    "Smith, N. J., & Levy, R. (2013). The effect of word predictability on reading time is logarithmic. *Cognition, 128*(3), 302‚Äì319. https://doi.org/10.1016/j.cognition.2013.02.013  \n",
    "\n",
    "Futrell, R., Staub, A., & Levy, R. (2024). Testing the predictions of surprisal theory in 11 languages. *Transactions of the Association for Computational Linguistics, 12*, 1258‚Äì1274. https://doi.org/10.1162/tacl_a_00612  \n",
    "\n",
    "Zhang, J., Liu, Y., Zhou, M., & Sun, M. (2024). Uncertainty quantification for in-context learning of large language models. *arXiv preprint.* https://arxiv.org/abs/2402.10189  \n",
    "\n",
    "Zhao, C., Wang, X., & Liu, Q. (2024). A survey on uncertainty quantification of large language models. *arXiv preprint.* https://arxiv.org/abs/2410.15326  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c2519c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
